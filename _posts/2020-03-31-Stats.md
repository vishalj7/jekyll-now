---
layout: post
title: Machine Learning Statistics
tags: Statistics   
---

### Stats You Need To Know For Machine Learning

This post will go through some of the required stats you should know for Machine Learning. This will cover many statistical areas but in a brief overview.
<br/><br/>


#### Variance and Covariance

Variance is the average of the squared deviation of a random variable from its mean. Basically, it measures how far a set of (random) numbers are spread out from their mean.

Covariance allows us to measure the strength of the association between two variables, if one variable moves in the same directions (up or down) as another then it is said to be a positive covariance. It is said to be negative covariance if the two variables move in different directions.

![an image alt text]({{ site.baseurl }}/images/stats/covariance.png "Positive and Negative Covariance")
source https://www.kdnuggets.com/2018/10/preprocessing-deep-learning-covariance-matrix-image-whitening.html
<br/><br/>


#### Standard deviation

Standard deviation (std) is the square root of the variance and this measures how much your data deviates from the mean. A low standard deviation means that the values are close to the mean and high standard deviation shows the values are far away from the mean. In a Gaussian (normal) distribution of data, typically 68% of the data tends to be 1 standard deviation away from the mean in either direction and about 95% of the data tends to be 2 standard deviations away from the mean (again either side of the mean).

![an image alt text]({{ site.baseurl }}/images/stats/standard_deviation_diagram.png "Standard Deviation on Normal Distribution")
source https://en.wikipedia.org/wiki/Standard_deviation#/media/File:Standard_deviation_diagram.svg
<br/><br/>


#### Correlation Coefficient

Correlation states the relationship between two variables and the correlation coefficient is a numerical measure of this relationship. The coefficient values is between -1 and 1 where -1 means a strong negative correlation so as one value of a variable goes up, the value of the other variable goes down. 1 means a strong positive correlation so as the values for one variable goes up so does it for the other variable. 0 means there is no correlation at all.

There are 2 main types, Pearson's and Spearman's correlation. 

Pearson's correrlation evaluates the linear relationship between two variables where if there is a change in one variable, a constant rate of change is found in the other variable. 

Spearman's correlation evaluates monotonic relationship between two variables where if there is a change in one variable, the change may not always be the same. Spearman's correlation coefficient is based on ranked values for each variable and not the actual data values.

![an image alt text]({{ site.baseurl }}/images/stats/correlation-coefficient.png "Correlation Coefficient")
source http://danshiebler.com/2017-06-25-metrics/
<br/><br/>


#### Central Limit Theorem

This is where means of multiple random samples start to resemble the normal distribution curve more and more as you increase the size of the samples used to calculate the mean. This still applies even if the original distribution of the data is not itself normally distributed. We can use the mean's normal distriubution to create confidence intervals, perform t-tests (see below for information about t-tests) and AVONA (again see below). 

![an image alt text]({{ site.baseurl }}/images/stats/central-limit-theorem.jpg "Central Limit Theorem")
source https://www.ncbi.nlm.nih.gov/books/NBK153593/figure/statisticalanalysis_figure7/
<br/><br/>


#### Bootstrapping

Bootstrapping is a resampling method in which random samples are taken from a dataset with replacement in order to estimate a statistic (e.g. mean, std) for the entire population. Replacement refers to the process that when a data value is selected it is placed back into the data pool to potentially be reselected. 

Bootstrapping is a used as a statistical inference method where using the random sampling to infer conclusions about the population. 

![an image alt text]({{ site.baseurl }}/images/stats/statistical-inference.gif "Statistical Inference")
source https://online.stat.psu.edu/stat414/node/18/
<br/><br/>


#### Confidence Intervals

Confidence Intervals uses confidence levels to show a range of where the true value may lie, the most common is 95%. Between the end-points of a 95% confidence interval, it   will contain the true value of what we are trying to predict with a probability of 0.95.

![an image alt text]({{ site.baseurl }}/images/stats/confidence_intervals.png "Confidence Intervals")
source https://www.spss-tutorials.com/confidence-intervals/
<br/><br/>


#### Null hypothesis

The hypothesis tests contain a null hypothesis and an alternative hypothesis. The null hypothesis is a general statement that what is observed between two groups is due to chance and nothing special is happening. This is the default assumption by the hypothesis test and it will only be rejected once there is evidence to suggest that the difference is not due to chance. 
<br/><br/>


#### P-value

The p-value is related to the null hypothesis as it is this value which is used to determine whether or not to reject the null hypothesis based on whether it is statistically significant (it ) when compared to threshold (usually 0.05). The p-value is the probability that your evidence supports the null hypothesis, the smaller the p-value means you have enough evidence to reject the null hypothesis but it also must be lower than the threshold. 
<br/><br/>

 
#### Z-score, T-test, F-test

A z-score is a numerical measure of a value's relationship to the mean of a group of values, this is measured in terms of standard deviation. Standard deviation is the amount of variablity from the mean within a dataset where z-score is the about of standard deviation from a data point (mean).

T-tests compare the values of the mean from two different samples and show how significant the differences are, it allows us to see if the differences where by chance. In simple terms, it determines whether the samples came from the same population. Every t-value has a p-value which is used to indicate the data difference didn't occur by chance.

F-test compares the mean of a groups of variables and it uses the f-statistic (a ratio) to determine if the groups are statistical significantly different. The F-statistic is used with the p-value which like before is the probability that the difference is by chance. 
<br/><br/>


#### Anova - analysis of variance

Anova (analysis of variance) is a test for see the statistical significance between two or more groups. It can use the F-statistic and there are two types: One way and Two way Anova.

One way Anova has only one independant variable but many categories to compare whereas Two way Anova has 2 independant variables. 
<br/><br/>


#### Chi-square test

There 2 chi-square tests, one is the goodness of fit test. This checks if a sample data fits the distribution of the population. Basically, it determines if the sample data is a good representative of the acutal population. 

The other chi-square test is for independence which compares two variables in a contingency table to see if they are related. It tests to see whether the distributions of categorical variables differ from each another so in ensence it see if their is relationship between two variables. A small chi squest test statistic means the observed data does fit the expected data well, so there is a relationship.

A large chi-square test statistic shows there is no relationship.
<br/><br/>


#### Least Squares

Least squares in used in linear regression to 'draw' a line of best fit where it tries to minimise the sum of the squared residual values (the distance from a point to the line of best fit) aka residual sum of squares. This method of minimising the sum of squared residuals is known as least squares regression or ordinary least sqaures (OLS) regression. 

![an image alt text]({{ site.baseurl }}/images/stats/least_squares.jpeg "Least Squares Regression")
source https://towardsdatascience.com/how-least-squares-regression-estimates-are-actually-calculated-662d237a4d7e
<br/><br/>

#### K-Fold Cross Validation 

Cross validation is a technique to authentic the results of a outcome, its aim is to see how a statistical metric will vary over different fragments of the input data. The idea is that running k-number of validations will help us to see if the model is overfitting and it also helps us to see how the model will perform on different datasets. The idea is to split the all of the data into smaller fragments which is then used to train and then test the model, when one fragment is used to train in another iteration that same fragment of data is used to test the model and every iteration a metric is used to identify the accuracy. The metric is averaged to get an overall accuracy of the model. The k represents the number of interations and fragments to create which tend to vary from 5 to 10.

![an image alt text]({{ site.baseurl }}/images/stats/cross-validation.png "K-Fold Cross Validation")
source https://medium.com/@sebastiannorena/some-model-tuning-methods-bfef3e6544f0
<br/><br/>

#### Precision, Recall and F1 score

To start we need to look at a confusion matrix which shows the number of correctly and incorrectly classed data points. This is typcially used for classification models where we have a positive and negative class. A positive class is something you want to predict e.g. spam email and a negative class is the opposite e.g. no spam, now we need to identify how many data points were correctly predicted as a positive class and negative class but there is the chance that the model incorrectly predicts a data record as positive or negative. We then get 4 types of outcomes:

+ True Positive - where the model correctly predicts the positive class
+ True Negative - where the model correctly predicts the negative class
+ False Positive - where the  model incorrectly predicts the positive class i.e. it was actual a negative class
+ False Negative - where the model incorrectly predicts the negative class i.e. it was actual a positive class

These outcomes form our confusion matrix.

![an image alt text]({{ site.baseurl }}/images/stats/confusion_matrix_1.png "Confusion Matrix")
source http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/
<br/><br/>

Now we are able to calculate the precision and recall. 

Precision tries to answer "What proportion of positive identifications was actually correct?" 

Recall tries to answer "What proportion of actual positives was identified correctly?"
<br/><br/>
![an image alt text]({{ site.baseurl }}/images/stats/precision_recall_formula.jpeg "Confusion Matrix")
source https://medium.com/@raghaviadoni/evaluation-metrics-i-precision-recall-and-f1-score-3ec25e9fb5d3

The F1 score is the weighted average of Precision and Recall which takes both false positives and false negatives into account. F1 is usually more useful than accuracy, especially if you have an uneven class distribution and there is a need to balance. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. 

![an image alt text]({{ site.baseurl }}/images/stats/f1-score.png "F1 Score")
source https://medium.com/@raghaviadoni/evaluation-metrics-i-precision-recall-and-f1-score-3ec25e9fb5d3
<br/><br/>

#### ROC curve and AUC

A ROC curve (receiver operating characteristic) curve is a graph which shows how a model performs by plotting the true positive rate (y-axis) and false positive rate (x-axis). It raises the classification threashold which increases the TPR (true positive rate) and FPR (false positive rate) and this is then plotted. 

![an image alt text]({{ site.baseurl }}/images/stats/roc.png "RoC")
source https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/
<br/><br/>

The graph is great but how do we put a number against it? AUC or area under the curve. 

AUC is a performance metric we can use to determine how well our model is able to identify positive and negative classes, the area under the graph is calculated to be between 0 and 1 where 1 is the greatest score. 

![an image alt text]({{ site.baseurl }}/images/stats/auc.png "AUC")
source https://medium.com/acing-ai/what-is-auc-446a71810df9
<br/><br/>


#### RMSE

RMSE (root mean square error) is a metric used in regression analysis, it is the average error between the predicted value and actual value. 
<br/><br/>

#### Expected value

The expected value for a random variable is each possible value the variable can be, multiplied by the probability of that value. The values and the probabilities are summed to produce the expected value.
<br/><br/>


#### Eigenvectors and Eigenvalues

Eigenvectors are vectors which don't change direction and are only scaled when a liner transformation is applied to it. The eigenvalues is the value required to scale the eigen vector. Eigenvectors are used in Principle Component Analysis (PCA) where eigenvectors and their associated eigenvalues are kept.
<br/><br/>

#### Entropy, Cross-Entropy & KL-Divergence 

Entropy is the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy. (a way to tell the unpredictability of the probability distribution)

Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. Average number of total bits to required represent event Q instead of P.

KL-Divergence is the average number of extra bits required to represent event Q instead of P.
<br/><br/>
